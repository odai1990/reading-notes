# Read 42
## Ethis
If you write code for a living, there’s a chance that at some point in your career, someone will ask you to code something a little deceitful – if not outright unethical.

This happened to me back in the year 2000. And it’s something I’ll never be able to forget.

I wrote my first line of code at 6 years old. I’m no prodigy though. I had a lot of help from my dad at the time. But I was hooked. I loved it.

By the time I was 15, I was working part-time for my dad’s consulting firm. I built websites and coded small components for business apps on weekends and in the summer.

I was woefully underpaid. But as my dad still likes to point out, I got free room and board, and some pretty valuable work experience.

Later, I managed to help fund a part of my education through a few freelance coding gigs. I built a couple of early e-commerce sites for some local small businesses.

By age 21, I managed to land a full-time coding job with an interactive marketing firm in Toronto, Canada.

The firm had been founded by a medical doctor and many of its clients were large pharmaceutical companies.

In Canada, there are strict limits on how pharmaceutical companies can advertise prescription drugs directly to consumers.

As a result, these companies would create websites that present general information about whatever symptoms their drugs were meant to address. Then, if a visitor could prove they had a prescription, they were given access to a patient portal with more specific info about the drug.

One of the projects I was assigned to involved a drug that was targeted at women. The graphics and general style of the website made it clear that the client wanted to specifically target teenage girls.

One of the features of this website was a quiz that asked girls a series of questions and recommended a type of drug based on their answers.

Remember, this website was posing as a general information site. It was not clearly an advertisement for any particular drug.

When I received the requirements, they contained the questions for the quiz, along with multiple choice answers for each question.

Missing from the requirements was any indication of what I should do with the answers at the end of the quiz. So what rules determined what treatment the quiz would recommend?

I spoke to the Account Manager about this. She emailed the client and got me the requirements. With those, I proceeded to code up the quiz.

Before submitting the website to the client, my project manager decided to give it a quick test. She tried the quiz, then came over to my desk:

“The quiz doesn’t work,” she said.
“Oh. What’s broken?” I asked.
“Well, it seems that no matter what I do, the quiz recommends the client’s drug as the best possible treatment. The only exception is if I say I’m allergic. Or if I say I am already taking it.”
“Yes. That’s what the requirements say to do. Everything leads to the client’s drug.”
“Oh. Okay. Cool.”
And she was off.

I wish I could tell you that when I first saw those requirements they bothered me. I wish I could tell you that it felt wrong to code something that was basically designed to trick young girls. But the truth is, I didn’t think much of it at the time. I had a job to do, and I did it.

Nothing that we were doing was illegal. As the youngest developer on my team, I was making good money for my age. And in the end, I understood that the real purpose of the site was to push a particular drug. So, I chalked this tactic up to “marketing.”

The client was extremely pleased with the site. So much so that their rep invited me and the entire team out to a fancy steak dinner.

The day of the dinner, shortly before leaving the office, a colleague emailed me a link to a news report online. It was about a young girl who had taken the drug I’d built the website for.

She had killed herself.

It turned out that among the main side effects of that drug were severe depression and suicidal thoughts.

The colleague who had emailed me didn’t show up to dinner.

I still went. It was difficult and awkward. I never mentioned the news report. I just ate my steak quietly and tried to force a smile when I could.

The next day, I called my sister. She was 19 at the time. We had discovered while working on the project that she had actually been prescribed the very drug I was building a site for.

When we first talked about it, we thought the whole thing was a neat coincidence. Now, the tone of our conversation was very different. I advised her to get off the drug ASAP. Thankfully, she listened.

There are a million and one ways for me to rationalize my part in later suicides and severe depression. Even today, there is ongoing litigation with former patients.

It’s easy to make an argument that I had no part in it at all. Still, I’ve never felt okay about writing that code.

Not long after that dinner, I resigned.

As developers, we are often one of the last lines of defense against potentially dangerous and unethical practices.

We’re approaching a time where software will drive the vehicle that transports your family to soccer practice. There are already AI programs that help doctors diagnose disease. It’s not hard to imagine them recommending prescription drugs soon, too.

The more software continues to take over every aspect of our lives, the more important it will be for us to take a stand and ensure that our ethics are ever-present in our code.

Since that day, I always try to think twice about the effects of my code before I write it. I hope that you will too.

These days, I spend a portion of my time writing and sending a regular newsletter that aims to help developers master their craft and advance their careers. Please sign up below to receive it.

Google is experiencing a “moral and ethical” crisis. That’s the view of hundreds of employees at the tech company, who are protesting the development of a censored search engine for internet users in China.

About 1,400 Google employees — out of the more than 88,000 — signed a letter to company executives this week, seeking more details and transparency about the project and demanding employee input in decisions about what kind of work Google takes on. They also expressed concern that the company is violating its own ethical principles.

“Currently we do not have the information required to make ethically-informed decisions about our work, our projects, and our employment,” they wrote in the letter, obtained by the Intercept and the New York Times.

The existence of the censored search tool — dubbed Dragonfly — was revealed earlier this month by the Intercept, sparking outcry within the company’s ranks and drawing harsh criticism from human rights groups across the world. Internal documents leaked to journalists described how the app-based search platform could block internet users in China from seeing web pages that discuss human rights, peaceful protests, democracy and other topics blacklisted by China’s authoritarian government.

Only a small group of Google engineers are reportedly developing the platform for Beijing, and information about the project has been so heavily guarded that only a few hundred Google employees even knew about it. Google has declined to comment publicly on Dragonfly, but Google CEO Sundar Pichai defended the project Thursday during a weekly staff meeting, saying that the project for China is merely in the “exploratory” stage.

The internal backlash among employees represents mounting concerns about whether Google has “lost its moral compass” in the corporate pursuit to enrich shareholders. But it also suggests that the people who make Google’s technology have more power in shaping corporate decisions than even shareholders have. In April, thousands of Google employees protested the company’s military contract with the Pentagon — known as project Maven — which developed technology to analyze drone video footage that could potentially identify human targets.

About a dozen engineers ended up resigning over what they viewed as an unethical use of artificial intelligence, prompting Google to let the contract expire in June, and leading executives to promise that they would never use AI technology to harm others.

The fact that Google employees succeeded in forcing one of the most powerful companies in the world to put ethics before shareholder value is a remarkable feat in corporate America, and signals why workers need an official voice in strategic decisions. Whether or not Google ultimately drops its plan to help China censor information will be a test of how far that power extends.

Doing business in China is good for shareholders, bad for humanity

It’s no mystery why Google executives want to do business with Chinese government officials: It’s profitable. With its population at 1.3 billion, China has the largest number of internet users in the world, so breaking into the Chinese market has been a long-time goal for Silicon Valley tech giants in their quest to find new users and to grow profits.

But working in China inevitably raises ethical issues for any US company. Doing business in mainland China means making deals with an authoritarian government that has a record of human rights abuses and a strict suppression of speech.

Despite this, Silicon Valley tech companies have shown a willingness to put aside their idealism — or rationalize their decisions to court Beijing. LinkedIn, for example, has a presence in China because it agreed to block certain online content.

Facebook is still banned in China, but chief executive Mark Zuckerberg has been trying to change that. In 2016, news surfaced that Facebook was building a censorship tool similar to Google’s Dragonfly project: It would allow a third-party to block certain Facebook posts in China in exchange for the government’s permission to operate the social media network there. A backlash similar to the Dragonfly controversy ensued, raising concerns about the potential for government officials to use the platform to spy on dissidents and punish them. These concerns led several Facebook employees who worked on the project to resign. That project was in early stages, too, and there’s no evidence that Facebook ever presented the tool to Chinese officials.

But Google’s decision to enter the Chinese market is more unnerving, for several reasons. It’s a striking reversal of the strong stance the company took back in 2010, when it decided to leave China in protest of Chinese government hacking and its crackdown on free speech. The decision also seems at odds with Google’s once-prominent motto “Don’t be evil” and it clashes with the principles the company adopted in June after the Pentagon contract controversy, in which Pichai promised that the company would not to use artificial intelligence to develop technology “whose purpose contravenes widely accepted principles of international law and human rights.”

Google employees say these kinds of promises are no longer enough, in light of the news about the censorship tool, and they are demanding a more formal role in decisions about the ethical implications of their work.

The push to make employees corporate stakeholders

For the past few decades, rank-and-file workers have had no real influence in how public companies invest profits or make decisions about new revenue streams.

Modern American capitalism has been driven by a singular mission: to bring value to the people who own company stock. Vox’s Matt Yglesias explains how this mentality leads executives to pursue profit above other worthwhile goals.
